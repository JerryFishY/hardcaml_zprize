{1 Zprize_ntt}

{{!Zprize_ntt}This library} provides a design which performs a single transform size configured at
build time. For the Zprize competition we target a transform of size 2{^24}.

{2 Algorithm}

The design is based around the 4-step algorithm which decomoses the full 2{^24}
NTT into multiple 2{^12} NTTs across columns and rows of a 2{^12} x 2{^12}
matrix. The 4-step algorithm is described in section 7.1 of
{{:https://arxiv.org/pdf/2011.11524.pdf} this paper}. Here's a summary of what
it is:

1. Layout the 2{^24} size input data as a 2{^12} X 2{^12} matrix in row-major
order (ie: [mat[i][j] = data[i * 2^12 + j]]).

2. Perform a length-2{^12} NTT along all columns and write the results back
in place

3. Multiply [mat[i][j]] by [x^{i * j}], where [x] is the N-th root of unity of
the underling field, and N = 2{^24}

4. Perform a length-2{^12} NTT along all rows and write the results back in place

5. Tranpose the matrix

The overall complexity (in terms of butterly operations performed) is roughly
equivalent to a single 2{^24} INNT, though an extra twiddle factor correction
pass (ie: step 3) is required between the column and row phases.

On the otherhand, onchip memory usage is drastically reduce, and it becomes possible to
implement multiple smaller INNT cores for improved performance through parallelism.

{2 Our Implementation}

We implemented our NTT core on top of the Vitis platform for Varium C1100. This
platform provides us with the PCIe and HBM interfaces. As such the design is
provided as Vitis kernels which are put together to provide the final
architecture.

There are 2 vitis kernels involved in our implementation:

- Hardcaml RTL Kernel implementing the core NTT algorithm
- C++ HLS Kernel which sequences PCIe and HBM memory accesses

Our implementation can be parameterized by the number of cores it supports -
the only requirement is it has a power of 2 and there must be at least 8 cores
(and subject to resource limits on the FPGA). 

Where C is the number of coress.

Our actual NTT implementation comprises of two phases

Phase 1: Performs steps 1, 2 and 3 of the algorithm
- The HLS kernel streams the first C columns via AXI Stream to the Hardcaml
  kernel from a HBM bank
- The Hardcaml RTL kernel accepts the NTT
- The HLS kernel concurrently writes the results of the into a different HBM
  Bank

Phase 2: Performs step 4 and step 5 of the algorithm simultaneously.
- The HLS kernel stream rows to the hardcaml kernel via Axi Stream
- The Hardcaml RTL kernel perform the NTT and sends the results back via Axi stream
- The HLS kernel concurrently writes it back in columns - this implicitly
  performs a matrix transpose without the dedicated step

Some minor optimizations that are included in our implementation:
- The NTT core double buffers the contents coming from memory, this means that
  while its performing a NTT, it can start accepting input for the next NTT.

{2 Memory Bandwidth and Streaming}

THe 4 step algorithm requires both a coloumn and row transform, with transposes between phases.
This is performed both by controlling the memory access pattern (normal layout build) or by
pre and post processing the input/output matrices (optimized layout builds).

One significant issue we have faced with this project is the bandwidth performance of HBM.
In normal layouts, we tend to burst 64 to 512 bytes before opening a new row.  The row open operation appears to be taking upto 200-250 HBM clock cycles (about 100 cycle at our internal 200 Mhz clock).
We had expected significantly better performance from HBM than this and lacked time to try
tuning varios HBM parameters to see if we could get better performance.

The optimized layouts use the host for pre/post processing and dramaticlly improve bandwidth
efficiency - the smallest transfers are now 2048 - 4096 bytes (which is only for one read
phase - the other read/write phases are completely linear).

We see tremendously improved through put of the core with this scheme, though we expect it to
be judged harshly in this competition due to the extra pre/post processing step.  We include
it none-the-less as it shows the potential performance we can get to with either a more optimised
HBM structure, or different memory architecture (like DDR4).

We will present the results for the normal layout (no data rearrangement) and
optimized layout (ie: with preprocessing data arrangement separately)

{2 Results}

We have tested our design and ran builds on a 6-core
Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz machine with
[Ubuntu 22.04 LTS (GNU/Linux 5.15.0-48-generic x86_64)]. We did not use
any special kernel flags / boot parameters to obatain our results.

We measure our latency by taking the fpga-only evaluation latency across 200
NTT runs. Power measured by sampling [xbutil examaine --report electrical --device
<device-pcie-id>] 10 times during latency benchmark

{3 Latency, Power and Resource Utilization}

The table below depicits our results for various builds

{[

|------------------------------------------------------------------------------|
|   Build | Latency(s) | Power(W) | LUTS   | Registers |  DSP | BRAM36 | URAM  |
|------------------------------------------------------------------------------|
|  8 core |     0.2315 |    16.97 | 107291 |    141006 |  260 |    162 |   48  |
| 16 core |     0.1238 |    18.19 | 126422 |    156149 |  512 |    162 |   96  |
| 32 core |     0.0691 |    21.13 | 166488 |    184436 | 1028 |    162 |   192 |
| 64 core |     0.0450 |    27.70 | 265523 |    246385 | 2052 |    162 |   384 |
|------------------------------------------------------------------------------|

]}

Here are the available resources on the FPGA.  Note that as we are building on
top of a vitis platform, it imposes a non-trivial fixed-cost that we don't
control. The number is reported as "fixed" in the post_route_utilization.rpt

{[

----------------------------------------------------------
| Resource  | Available on FPGA | Used by Vitis Platform |
----------------------------------------------------------
|     LUTS  |            870720 |                  62191 |
| Registers |           1743360 |                  81502 |
|       DSP |              5952 |                      4 |
|    BRAM36 |              1344 |                      0 |
|      URAM |               640 |                      0 |
----------------------------------------------------------

]}

{3 FOM Measurement}

Here are our FOM numbers. As detailed in the evaluation criteria given to us,
FOM is computed as [latency * sqrt(Power) * U_norm]

Latency and Power is used as report above in seconds and Watts respectively.
We calculate [U_norm = U(LUTS) + U(Registers) + U(DSP) + U(BRAM) + U(URAM)].
The max possible value of [U_norm] is hence 4.0, since 0 <= U(.) < 1.0

{[

These are FOM numbers assuming we don't include the platform (aka fixed resources)
in our utlization

--------------------------------------------------------------------
|             Utilization (%)                    |        |        |
|   LUTs |  Registers |    DSP |   BRAM |   URAM | U_norm |  FOM   |
--------------------------------------------------------------------
| 0.0518 |     0.0341 | 0.0430 | 0.1205 | 0.0750 | 0.3245 | 0.3095 |
| 0.0749 |     0.0428 | 0.0860 | 0.1205 | 0.1500 | 0.4743 | 0.2505 |
| 0.1198 |     0.0590 | 0.1720 | 0.1205 | 0.3000 | 0.7714 | 0.2451 |
| 0.2335 |     0.0946 | 0.3441 | 0.1205 | 0.6000 | 1.3927 | 0.3301 |
--------------------------------------------------------------------

These are FOM Numbers assuming we have to include the platform resources as
part of our utilization. To stress this fact -- we don't think those resources
should be considered as part of the evaluation!

--------------------------------------------------------------------
|             Utilization (%)                    |        |        |
|   LUTs |  Registers |    DSP |   BRAM |   URAM | U_norm |  FOM   |
--------------------------------------------------------------------
| 0.1232 |     0.0809 | 0.0437 | 0.1205 | 0.0750 | 0.4433 | 0.4229 |
| 0.1463 |     0.0896 | 0.0867 | 0.1205 | 0.1500 | 0.5931 | 0.3132 |
| 0.1912 |     0.1058 | 0.1727 | 0.1205 | 0.3000 | 0.8903 | 0.2829 |
| 0.3049 |     0.1413 | 0.3448 | 0.1205 | 0.6000 | 1.5116 | 0.3583 |
--------------------------------------------------------------------

]}

Our best-build for the evaluation criteria is the 32-core variant, with a {b
FOM of 0.2451} (or 0.2829 using the less desirable criteria)

{2 A note about Optimizing layout}

{2 Throughput}

We now show the result of 6 designs which target differing parallelism and memory architectures.

Note that each core instantiated has a throughput of approximately one INNT per second at
our target 200 Mhz.

{%html:
<table class="perf">
<caption>Normal memory access pattern</caption>
  <tr>
    <td>Cores</td>
    <td>PCIe down</td>
    <td>NTT processing</td>
    <td>PCIe up</td>
  </tr>
  <tr>
    <td>8</td>
    <td>0.039</td>
    <td>0.23</td>
    <td>0.054</td>
  </tr>
  <tr>
    <td>16</td>
    <td>x0.0410</td>
    <td>0.123</td>
    <td>0.056</td>
  </tr>
  <tr>
    <td>32</td>
    <td>0.042</td>
    <td>0.068</td>
    <td>0.056</td>
  </tr>
  <tr>
    <td>64</td>
    <td>0.041</td>
    <td>0.044</td>
    <td>0.056</td>
  </tr>
</table>
%}

{%html:
<table class="perf">
<caption>Optimized memory access pattern</caption>
  <tr>
    <td>Cores</td>
    <td>Preprocessing</td>
    <td>PCIe down</td>
    <td>NTT processing</td>
    <td>PCIe up</td>
    <td>Postprocessing</td>
  </tr>
  <tr>
    <td>32</td>
    <td>0.0218</td>
    <td>0.0417</td>
    <td>0.0348</td>
    <td>0.0555</td>
    <td>0.0228</td>
  </tr>
  <tr>
    <td>64</td>
    <td>0.0213</td>
    <td>0.0414</td>
    <td>0.0267</td>
    <td>0.0552</td>
    <td>0.0231</td>
  </tr>
</table>
%}

{2 Resource usage}

{%html:
<table class="perf">
<caption>Normal memory access pattern</caption>
  <tr>
    <td>Cores</td>
    <td>LUTs</td>
    <td>REGs</td>
    <td>DSPs</td>
    <td>BRAM</td>
    <td>URAM</td>
  </tr>
  <tr>
    <td>8</td>
    <td>169482</td>
    <td>222508</td>
    <td>264</td>
    <td>162</td>
    <td>48</td>
  </tr>
  <tr>
    <td>16</td>
    <td>189613</td>
    <td>237651</td>
    <td>520</td>
    <td>162</td>
    <td>96</td>
  </tr>
  <tr>
    <td>32</td>
    <td>228679</td>
    <td>265938</td>
    <td>1032</td>
    <td>162</td>
    <td>192</td>
  </tr>
  <tr>
    <td>64</td>
    <td>327714</td>
    <td>327887</td>
    <td>2052</td>
    <td>162</td>
    <td>384</td>
  </tr>
</table>
%}

{%html:
<table class="perf">
<caption>Optimized memory access pattern</caption>
  <tr>
    <td>Cores</td>
    <td>LUTs</td>
    <td>REGs</td>
    <td>DSPs</td>
    <td>BRAM</td>
    <td>URAM</td>
  </tr>
  <tr>
    <td>32</td>
    <td>166270</td>
    <td>183713</td>
    <td>1028</td>
    <td>162</td>
    <td>192</td>
  </tr>
  <tr>
    <td>64</td>
    <td>265308</td>
    <td>243870</td>
    <td>2052</td>
    <td>162</td>
    <td>384</td>
  </tr>
</table>
%}

{2 Build and Testing Instructions}

Please refer {{!page:zprize_ntt_build_instructions} Build Instructions}
